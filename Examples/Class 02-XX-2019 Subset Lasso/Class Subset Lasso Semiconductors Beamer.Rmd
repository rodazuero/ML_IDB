---
title: "Machine Learning for All: Examples for Subset Selection & Lasso"
author: "Anastasiya Yarygina"
date: "Monday, January 21, 2019"
output: beamer_presentation
---



```{r, echo=FALSE}
# Clear workig space
rm(list = ls())

# SET working directory
# setwd("your working directory")
```




## Practical Excercises

* Subset selection
    + \href{https://www.mailman.columbia.edu/research/population-health-methods/false-discovery-rate}{\color{blue}{False Discovery Rate (FDR)}}^[FDR is the expected proportion of false positives] as a selection tool
    + In-sample and Out of Sample (OOS) fit
    + Forward stepwise Regression
* Regularization
    + LASSO Regularization Path
    + Parameter selection using Cross Validation
* Data: \href{https://rpubs.com/jeff_datascience/Semiconductor_Manufacturing}{\color{blue}{Semiconductors dataset}}




## Semiconductors dataset: explore predictors

* This dataset has 201 predictors 
* Some p-values are clustered at zero. But which are **significant signals**?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}
# load data
SC <- read.csv("semiconductor.csv")

# Info about this dataset
# str(SC)
# cat("Dimensions of SC dataset")
# dim(SC)

## fit full model, LL
full <- glm(FAIL ~ ., data=SC, family=binomial)
#summary(full)

# summary of the fit
# cat("R2=1-dev(Beta)/dev(Beta=0)")
# 1 - full$deviance/full$null.deviance

# take non-intercept p-values from a glm
# -1 to drop the intercept 
#  4 is 4th column
pvals <- summary(full)$coef[-1,4] 

# plot pvalues
hist(pvals, xlab="p-value", main="", col="lightblue")
```





## Select predictors using FDR 

* How many predictors are in fact good signals (q=10% FDR)? 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}
## At 10% FDR, we get 25 significant coefficients
source("fdr.R")
q <- 0.1
alpha <- fdr_cut(pvals, q) ## cutoff
#alpha
signif <- which(pvals <= alpha) ## which are significant
#signif
cat("The nubmer of significant signals: ")
length(signif)  # the number of significant
```




## Compare **in-sample** fit of **full** and **cut** models 

* Fit a new **cut** model using only 25 best signals
* How does the in-sample fit change? 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}
## At 10% FDR, we get 25 significant coefficients
# Re-run a cut regression using only these 25 predictors
# get names of variables to include
cutvar <- c("FAIL", rownames(summary(full)$coef)[-1][signif]) 
# str(cutvar) # list of characters

# run regression on these 25 alone
# full <- glm(FAIL ~ ., data=SC,          family="binomial")
cut <- glm(FAIL ~ ., data=SC[,cutvar], family="binomial")

# new in-sample R2: drops to 
# 1 - cut$deviance/cut$null.deviance

# compare full model R2 and cut model R2
cat("Full model R2=", 1 - full$deviance/full$null.deviance)
cat("Cut model R2=", 1 - cut$deviance/cut$null.deviance)
```



## Compare **OOS** fit of **full** and **cut** models 

Split data in 10 random samples, fit **full** and **cut** models on 9 samples, predict on 10th. What are the average $R^2$?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}
# Out of sample prediction 
# for this we call our deviance function
source("deviance.R")
set.seed(31)

n <- nrow(SC) # the number of observations

# split the data into 10 randon subsets (folds)
nfold <- 10 # the number of OOS validation `folds'

# create a vector of fold memberships (random order)
foldid <- rep(1:nfold,each=ceiling(n/nfold))[sample(1:n)]

# create an empty dataframe of results
OOS <- data.frame(full=rep(NA,nfold), cut=rep(NA,nfold)) 

# use a for loop to run through the nfold trails
# for nfold = 10, we fit model using 9/10 of data and recor
# R2 on the left-out subset.
for(k in 1:nfold){ 
  train <- which(foldid!=k) # train on all but fold `k'
		
	# fit the two regressions, full and cut
	rfull <- glm(FAIL~., data=SC, subset=train, family=binomial)
	rcut <- glm(FAIL~., data=SC[,cutvar], subset=train, family=binomial)

	# get predictions: type=response so we have probabilities
	predfull <- predict(rfull, newdata=SC[-train,], type="response")
	predcut <- predict(rcut, newdata=SC[-train,], type="response")

	# calculate and log R2
	OOS$full[k] <- R2(y=SC$FAIL[-train], pred=predfull, family="binomial")
	OOS$cut[k] <- R2(y=SC$FAIL[-train], pred=predcut, family="binomial")

	# print progress (number of folds)
	# print(k)
}

# look at the OOS data frame
# str(OOS)

# plot it in plum
boxplot(OOS, col="plum", ylab="R2", xlab="model")

# what are the average OOS R2?
# colMeans(OOS) # Full model is really bad! 
```

Cut model mean OOS $R^2$ is about 1/2 in-sample $R^2$. Full model is terrible! 




## Forward Stepwise Regression 

1. Fit all univariate models. Choose the one with the highest $R^2$, keep this variable, say X1, for your final model.
2. Fit all bivariate models including X1, choose the one with the highest $R^2$, keep two variables, say, X1 and X15.
3. Repeat: max $R^2$ by adding one variable at time to the model. 
4. Stop when AIC is lower for the current model than for any of the models that add one variable.

The Forward Stepwise procedure chooses around 70 coefficients.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}
# A forward stepwise procedure
# null model
null <- glm(FAIL~1, data=SC)
# summary (null)

# forward stepwise regression 
# how does it work?
# 1. fit all univariate models. choose the one
# with the highest R2, keep this variable, say X1,
# for our final model
# 2. fit all bivariate models including X1, 
# choose the one with the highest R2, keep two variables, 
# say, X1 and X15.
# 3. Repeat: maximize R2 by adding one variable to the model. 
# 4. Stop when AIC is lower for the current model than for any 
# of the models that add one variable

# look at what's inside of step function
#?step

# forward stepwise procedure takes time
fwd <- step(null, scope=formula(full),
                        dir="forward", trace=0, steps=5)
# cat("Number fo chosen coefficients: ")
# length(coef(fwd)) # chooses around 70 coefficients
# coef(fwd)
```




## Regularization using LASSO^[Least Absolute Shrinkage and Selection Operator]

* Depart from optimality: 
    - minimize deviance + **cost on an absolute size** of coefficients

* By penalizing we **shrink** some **estimates towards zero**

* Some coefficients can become zero and get eliminated from the model

* Tunning parameter **$\lambda$** is the **amount of shrinkage** 




## LASSO Algorithm using *gamlr*^[Gamma-Lasso regression] package 

* Start with large $\lambda_1$ so that $\hat{\beta} = 0$ 
* For $t = 2...T$ update  $\hat{\beta}$ to be optimal under $\lambda_t<\lambda_{t-1}$
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}

# Regularization: minimize deviance + cost
# Idea: for given beta, the cost is deviance plus a penalty 
# if beta is away from zero.

if (!require("gamlr")) install.packages("gamlr") # install if necessary
library ("gamlr") # load the package to estimate lasso model
# ? gamlr

# note: by default, all variables in gamlr are standardized
# do not set standardize=FLASE unless you have very good reasons to do so

# gamlr does not have "formula" interface, i.e., we cannot write (y~predictors, data=DATA)
# We have to pass predictors as a sparse matrix. 
# We use sparse.model.matrix function to transform or X in sparse matrix.

scx <- sparse.model.matrix(FAIL ~ ., data=SC)[,-1] # do -1 remove FAIL
# str(scx)
# dim(scx)
# dim(SC)

# here, we could have also just done x <- as.matrix(SC[,-1]).
# but sparse.model.matrix is a good way of doing things if you have factors.

scy <- SC$FAIL # pull out FAIL form SC for convenience

# fit a single lasso
sclasso <- gamlr(scx, scy, family="binomial")

# path plot
plot(sclasso) 

# first 6 rows of the summary table
# head(summary(sclasso))
```

\tiny
At the top of the figure: number of non-zero coefficients
\normalsize





## Cross Validation using *gamlr*

* Set a sequence of $\lambda_1, ...,\lambda_T$
* For each $k=1,..., K$ folds:
    - Fit the path on all data except fold $k$
    - Get fitted deviance on left-out data
* Select $\lambda$ that gives minimum average OOS deviance


## Cross Validation using *gamlr*

    
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}
## cross validated lasso (verb prints progress)
sccvl <- cv.gamlr(scx, scy, family="binomial", verb=FALSE)
plot(sccvl, bty="n", xlim=c(-8,-3))

# AICc selected lambda is marked on the path plot
scbeta <- coef(sclasso) 
#sum(scbeta!=0) # chooses 30 (+intercept) 
#log(sclasso$lambda[which.min(AICc(sclasso))]) # at log(lambda) = -4.5
```



## Compare AICc selection and Cross Validation selection 

* Compare $\log(lambda)$ under different selection criteria
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=10}
## log lambdas selected under various criteria
cat("AICc: ", log(sclasso$lambda[which.min(AICc(sclasso))]))
cat("AICc chooses ", sum(scbeta!=0), "coefficients")

cat("------------------------------------------- " )
cat("Min deviance: ",log(sccvl$lambda.min))  # log(lambda) = -4.18
scb.min <- coef(sccvl, select="min")
cat("Min deviance chooses",sum(scb.min!=0) , "coefficients") # 11 + intercept selected


# CV 1se selection (the default)
scb.1se <- coef(sccvl)
#log(sccvl$lambda.1se)
#sum(scb.1se!=0) ## usually selects all zeros (just the intercept)
```


